{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01bf454",
   "metadata": {},
   "source": [
    "# Universal Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f2c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import List, Dict, Set, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb2b2e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72020d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"luyaojie/uie-large-en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8800e",
   "metadata": {},
   "source": [
    "Define special tokens based on UIE's T5 usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48afce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD_START = \"<extra_id_0>\"\n",
    "RECORD_END = \"<extra_id_1>\"\n",
    "SPAN_START = \"<extra_id_0>\" # start of type/label name or association list\n",
    "SPAN_END = \"<extra_id_1>\"   # end of type/label name or association list\n",
    "TEXT_START = \"<extra_id_5>\" # separator between type/label and span text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a3451",
   "metadata": {},
   "source": [
    "Define Label Mappers for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIO_LABEL_MAPPERS = {\n",
    "    \"biored\": {\n",
    "        \"entities\": {\n",
    "            \"GeneOrGeneProduct\": \"gene or gene product\",\n",
    "            \"DiseaseOrPhenotypicFeature\": \"disease or phenotypic feature\",\n",
    "            \"ChemicalEntity\": \"chemical entity\",\n",
    "            \"SequenceVariant\": \"sequence variant\",\n",
    "            \"Species\": \"species\",\n",
    "            \"CellLine\": \"cell line\",\n",
    "        },\n",
    "        \"relations\": {\n",
    "            \"Association\": \"is associated with\",\n",
    "            \"Positive_Correlation\": \"positively correlates with\",\n",
    "            \"Negative_Correlation\": \"negatively correlates with\",\n",
    "            \"Bind\": \"binds to\",\n",
    "            \"Cotreatment\": \"is cotreated with\",\n",
    "            \"Comparison\": \"is compared to\",\n",
    "            \"Conversion\": \"converts to\",\n",
    "            \"Drug_Interaction\": \"interacts with drug\",\n",
    "        }\n",
    "    },\n",
    "    \"ddi\": {\n",
    "        \"entities\": {\n",
    "            \"DRUG\": \"drug\",\n",
    "            \"GROUP\": \"group of drugs\",\n",
    "        },\n",
    "        \"relations\": {\n",
    "            \"MECHANISM\": \"has mechanism\",\n",
    "            \"EFFECT\": \"has effect\",\n",
    "            \"ADVISE\": \"is advised against\",\n",
    "            \"INT\": \"interacts with\",\n",
    "        }\n",
    "    },\n",
    "    \"chemprot\": {\n",
    "         \"entities\": {\n",
    "            \"CHEMICAL\": \"chemical\",\n",
    "            \"GENE\": \"gene or protein\",\n",
    "            \"GENE-Y\": \"gene or protein\",\n",
    "            \"GENE-N\": \"gene or protein\",\n",
    "        },\n",
    "        \"relations\": {\n",
    "            # Only include relations evaluated in BioCreative VI\n",
    "            \"CPR:3\": \"upregulates or activates\",\n",
    "            \"CPR:4\": \"downregulates or inhibits\",\n",
    "            \"CPR:5\": \"acts as agonist\",\n",
    "            \"CPR:6\": \"acts as antagonist\",\n",
    "            \"CPR:9\": \"is substrate or product of\",\n",
    "        }\n",
    "    },\n",
    "    \"bc5cdr\": {\n",
    "        \"entities\": {\n",
    "            \"Chemical\": \"chemical\",\n",
    "            \"Disease\": \"disease\",\n",
    "        },\n",
    "        \"relations\": {\n",
    "            \"CID\": \"causes or induces\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81c4ec",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads a JSONL file into a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def build_ssi(entity_types: List[str], relation_types: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Builds the Structured Schema Instructor (SSI) string.\n",
    "    Uses <spot> for entities and <asoc> for relations.\n",
    "\n",
    "    Args:\n",
    "        entity_types: A sorted list of descriptive entity type names.\n",
    "        relation_types: A sorted list of descriptive relation type names.\n",
    "\n",
    "    Returns:\n",
    "        The SSI string to prefix to the input text.\n",
    "    \"\"\"\n",
    "    ssi = \"<spot> \" + \"<spot> \".join(entity_types)\n",
    "    ssi += \" <asoc> \" + \"<asoc> \".join(relation_types) # Add space before <asoc>\n",
    "    ssi += \" <extra_id_2>\"\n",
    "    return ssi\n",
    "\n",
    "def parse_sel_string(sel_string: str) -> Dict[str, List[Tuple[str, List[Tuple[str, str]]]]]:\n",
    "    \"\"\"\n",
    "    Parses the cleaned generated SEL string into a structured format using defined markers.\n",
    "    Output: { \"entity_type\": [ (span, [(relation, object_span), ...]), ... ], ... }\n",
    "    \"\"\"\n",
    "    structured_output = {}\n",
    "\n",
    "    # The entire SEL is wrapped in RECORD_START/END\n",
    "    if sel_string.startswith(RECORD_START):\n",
    "        sel_string = sel_string[len(RECORD_START):]\n",
    "    \n",
    "    if sel_string.endswith(RECORD_END):\n",
    "        sel_string = sel_string[:-len(RECORD_END)]\n",
    "    \n",
    "    sel_string = sel_string.strip() # remove any extra spaces after stripping\n",
    "\n",
    "    # Split into individual record blocks\n",
    "    # Each block starts with RECORD_START and ends with RECORD_END\n",
    "    record_blocks = []\n",
    "    current_pos = 0\n",
    "    start_index = sel_string.find(RECORD_START)\n",
    "\n",
    "    # Handle cases where the string might not contain any blocks or is malformed\n",
    "    if start_index == -1 and len(sel_string) > 0:\n",
    "        # If no RECORD_START but content exists\n",
    "        print(f\"Warning: SEL string may be malformed or missing record markers: {sel_string}\")\n",
    "        return {} # Return empty as structure is unexpected\n",
    "\n",
    "    while start_index != -1:\n",
    "        end_index = sel_string.find(RECORD_END, start_index + len(RECORD_START))\n",
    "        if end_index == -1:\n",
    "            # Malformed - found a start but no end\n",
    "            print(f\"Warning: Malformed SEL block (no RECORD_END): {sel_string[start_index:]}\")\n",
    "            break # Stop processing the string\n",
    "        \n",
    "        # Extract the content between RECORD_START and RECORD_END\n",
    "        block_content = sel_string[start_index + len(RECORD_START):end_index].strip()\n",
    "        if block_content: # Only add non-empty blocks\n",
    "             record_blocks.append(block_content)\n",
    "             \n",
    "        # Find the start of the next block\n",
    "        start_index = sel_string.find(RECORD_START, end_index + len(RECORD_END))\n",
    "\n",
    "    # Process each block\n",
    "    for block in record_blocks:\n",
    "        try:\n",
    "            # Extract entity type\n",
    "            type_start = block.find(SPAN_START)\n",
    "            type_text_sep = block.find(TEXT_START)\n",
    "            if not (type_start == 0 and type_text_sep > type_start):\n",
    "                 print(f\"Warning: Could not find entity type in block: {block}\")\n",
    "                 continue # Skip malformed block\n",
    "            \n",
    "            entity_type = block[type_start + len(SPAN_START):type_text_sep].strip()\n",
    "\n",
    "            # Extract entity span and relations (if any)\n",
    "            remaining_block = block[type_text_sep + len(TEXT_START):].strip()\n",
    "            \n",
    "            # Check if there are associations after the main span\n",
    "            assoc_list_start = remaining_block.find(SPAN_START)\n",
    "            \n",
    "            entity_span = \"\"\n",
    "            relations = []\n",
    "\n",
    "            if assoc_list_start != -1:\n",
    "                # Relations exist\n",
    "                entity_span = remaining_block[:assoc_list_start].strip()\n",
    "                assoc_list_str = remaining_block[assoc_list_start:].strip()\n",
    "\n",
    "                # The association list should be wrapped in SPAN_START/SPAN_END\n",
    "                if assoc_list_str.startswith(SPAN_START) and assoc_list_str.endswith(SPAN_END):\n",
    "                    assoc_list_content = assoc_list_str[len(SPAN_START):-len(SPAN_END)].strip()\n",
    "                    \n",
    "                    # Split individual associations (each starts with SPAN_START)\n",
    "                    assoc_parts = []\n",
    "                    part_start = assoc_list_content.find(SPAN_START)\n",
    "                    while part_start != -1:\n",
    "                        part_end = assoc_list_content.find(SPAN_END, part_start + len(SPAN_START))\n",
    "                        if part_end == -1: break\n",
    "                        assoc_parts.append(assoc_list_content[part_start:part_end + len(SPAN_END)])\n",
    "                        part_start = assoc_list_content.find(SPAN_START, part_end + len(SPAN_END))\n",
    "\n",
    "                    for part in assoc_parts:\n",
    "                         # Inside each part: SPAN_START rel_type TEXT_START obj_span SPAN_END\n",
    "                         part_content = part[len(SPAN_START):-len(SPAN_END)].strip()\n",
    "                         rel_sep = part_content.find(TEXT_START)\n",
    "                         if rel_sep != -1:\n",
    "                             rel_type = part_content[:rel_sep].strip()\n",
    "                             obj_span = part_content[rel_sep + len(TEXT_START):].strip()\n",
    "                             if rel_type and obj_span:\n",
    "                                 relations.append((rel_type, obj_span))\n",
    "                else:\n",
    "                    print(f\"Warning: Malformed association list: {assoc_list_str}\")\n",
    "            else:\n",
    "                # No relations, the rest is the entity span\n",
    "                entity_span = remaining_block\n",
    "\n",
    "            # Add to output if valid\n",
    "            if entity_type and entity_span:\n",
    "                if entity_type not in structured_output:\n",
    "                    structured_output[entity_type] = []\n",
    "                # Avoid adding duplicates within the same record block parsing\n",
    "                current_entry = (entity_span, relations)\n",
    "                is_duplicate = False\n",
    "                for existing_span, existing_rels in structured_output[entity_type]:\n",
    "                    if existing_span == entity_span and set(existing_rels) == set(relations):\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "                if not is_duplicate:\n",
    "                    structured_output[entity_type].append(current_entry)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing block: {block}\\nError: {e}\")\n",
    "            continue # Skip block on error\n",
    "\n",
    "    return structured_output\n",
    "\n",
    "def get_ground_truth_sets(data_sample: Dict, entity_map: Dict, relation_map: Dict) -> Tuple[Set[Tuple[str, str]], Set[Tuple[str, str, str]]]:\n",
    "    \"\"\"Extracts ground truth entities and relations into sets for easy comparison.\"\"\"\n",
    "    gt_entities = set()\n",
    "    gt_relations = set()\n",
    "\n",
    "    # Map entity types using the mapper\n",
    "    entity_spans = {} # Store span -> canonical type\n",
    "    for entity in data_sample.get('entities', []):\n",
    "        raw_type = entity['type']\n",
    "        if raw_type in entity_map:\n",
    "            canonical_type = entity_map[raw_type]\n",
    "            span = entity['text']\n",
    "            gt_entities.add((span, canonical_type))\n",
    "            entity_spans[span] = canonical_type # Track for relation mapping\n",
    "\n",
    "    # Map relation types using the mapper\n",
    "    for relation in data_sample.get('relations', []):\n",
    "        raw_type = relation['type']\n",
    "        if raw_type in relation_map:\n",
    "            canonical_rel_type = relation_map[raw_type]\n",
    "            head_span = relation['head']['text']\n",
    "            tail_span = relation['tail']['text']\n",
    "\n",
    "            gt_relations.add((head_span, canonical_rel_type, tail_span))\n",
    "\n",
    "    return gt_entities, gt_relations\n",
    "\n",
    "\n",
    "def get_predicted_sets(parsed_sel: Dict) -> Tuple[Set[Tuple[str, str]], Set[Tuple[str, str, str]]]:\n",
    "    \"\"\"Extracts predicted entities and relations from the parsed SEL structure.\"\"\"\n",
    "    pred_entities = set()\n",
    "    pred_relations = set()\n",
    "\n",
    "    for entity_type, span_list in parsed_sel.items():\n",
    "        for entity_span, relations in span_list:\n",
    "            pred_entities.add((entity_span, entity_type))\n",
    "            for rel_type, obj_span in relations:\n",
    "                pred_relations.add((entity_span, rel_type, obj_span))\n",
    "\n",
    "    return pred_entities, pred_relations\n",
    "\n",
    "def calculate_extraction_metrics(preds: List[Tuple[Set, Set]], golds: List[Tuple[Set, Set]]) -> Dict:\n",
    "    \"\"\"Calculates P/R/F1 for entities and relations.\"\"\"\n",
    "    total_ent_tp, total_ent_fp, total_ent_fn = 0, 0, 0\n",
    "    total_rel_tp, total_rel_fp, total_rel_fn = 0, 0, 0\n",
    "\n",
    "    for (pred_ents, pred_rels), (gold_ents, gold_rels) in zip(preds, golds):\n",
    "        # Entity metrics\n",
    "        total_ent_tp += len(pred_ents.intersection(gold_ents))\n",
    "        total_ent_fp += len(pred_ents.difference(gold_ents))\n",
    "        total_ent_fn += len(gold_ents.difference(pred_ents))\n",
    "\n",
    "        # Relation metrics\n",
    "        total_rel_tp += len(pred_rels.intersection(gold_rels))\n",
    "        total_rel_fp += len(pred_rels.difference(gold_rels))\n",
    "        total_rel_fn += len(gold_rels.difference(pred_rels))\n",
    "\n",
    "    ent_precision = total_ent_tp / (total_ent_tp + total_ent_fp) if (total_ent_tp + total_ent_fp) > 0 else 0\n",
    "    ent_recall = total_ent_tp / (total_ent_tp + total_ent_fn) if (total_ent_tp + total_ent_fn) > 0 else 0\n",
    "    ent_f1 = 2 * (ent_precision * ent_recall) / (ent_precision + ent_recall) if (ent_precision + ent_recall) > 0 else 0\n",
    "\n",
    "    rel_precision = total_rel_tp / (total_rel_tp + total_rel_fp) if (total_rel_tp + total_rel_fp) > 0 else 0\n",
    "    rel_recall = total_rel_tp / (total_rel_tp + total_rel_fn) if (total_rel_tp + total_rel_fn) > 0 else 0\n",
    "    rel_f1 = 2 * (rel_precision * rel_recall) / (rel_precision + rel_recall) if (rel_precision + rel_recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"entity_precision\": ent_precision,\n",
    "        \"entity_recall\": ent_recall,\n",
    "        \"entity_f1\": ent_f1,\n",
    "        \"relation_precision\": rel_precision,\n",
    "        \"relation_recall\": rel_recall,\n",
    "        \"relation_f1\": rel_f1,\n",
    "    }\n",
    "\n",
    "def calculate_metrics_from_files(predictions_dir: str):\n",
    "    \"\"\"\n",
    "    Reads prediction files from the directory and calculates metrics for each.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Calculating Metrics from Saved Files in '{predictions_dir}' ---\")\n",
    "    all_dataset_metrics = {}\n",
    "\n",
    "    if not os.path.exists(predictions_dir):\n",
    "        print(f\"Predictions directory '{predictions_dir}' not found.\")\n",
    "        return {} # Return empty dict if directory doesn't exist\n",
    "\n",
    "    found_files = False\n",
    "    for filename in os.listdir(predictions_dir):\n",
    "        if filename.endswith(\"_predictions.jsonl\"):\n",
    "            found_files = True\n",
    "            dataset_name = filename.replace(\"_predictions.jsonl\", \"\")\n",
    "            filepath = os.path.join(predictions_dir, filename)\n",
    "            print(f\"\\nCalculating metrics for: {dataset_name} from {filepath}\")\n",
    "\n",
    "            all_preds_sets = []\n",
    "            all_labels_sets = []\n",
    "            line_count = 0\n",
    "\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        line_count += 1\n",
    "                        try:\n",
    "                            data = json.loads(line)\n",
    "                            # Convert saved lists back to sets of tuples\n",
    "                            pred_entities = set(tuple(e) for e in data['predicted_entities'])\n",
    "                            pred_relations = set(tuple(r) for r in data['predicted_relations'])\n",
    "                            gt_entities = set(tuple(e) for e in data['ground_truth_entities'])\n",
    "                            gt_relations = set(tuple(r) for r in data['ground_truth_relations'])\n",
    "\n",
    "                            all_preds_sets.append((pred_entities, pred_relations))\n",
    "                            all_labels_sets.append((gt_entities, gt_relations))\n",
    "                        except json.JSONDecodeError:\n",
    "                             print(f\"Skipping malformed JSON line {line_count} in {filename}\")\n",
    "                        except KeyError as e:\n",
    "                             print(f\"Skipping line {line_count} due to missing key {e} in {filename}\")\n",
    "\n",
    "                if not all_preds_sets:\n",
    "                    print(f\"No valid predictions found in file for {dataset_name}.\")\n",
    "                    continue\n",
    "\n",
    "                metrics = calculate_extraction_metrics(all_preds_sets, all_labels_sets)\n",
    "                all_dataset_metrics[dataset_name] = metrics\n",
    "                print(f\"Metrics for {dataset_name} ({len(all_preds_sets)} examples):\")\n",
    "                print(json.dumps(metrics, indent=2))\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                 print(f\"File not found during metric calculation: {filepath}\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error reading or processing file {filepath}: {e}\")\n",
    "\n",
    "    if not found_files:\n",
    "        print(f\"No prediction files found in '{predictions_dir}'.\")\n",
    "\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    return all_dataset_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9049169",
   "metadata": {},
   "source": [
    "## Zero-Shot Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740c570",
   "metadata": {},
   "source": [
    "Dataset Class for Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, ssi_string, max_source_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ssi_string = ssi_string\n",
    "        self.max_source_length = max_source_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        input_text = f\"{self.ssi_string} {text}\" # Add space after SSI\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_source_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Squeeze to remove batch dim added by tokenizer\n",
    "        return {\n",
    "            \"input_ids\": tokenized.input_ids.squeeze(0),\n",
    "            \"attention_mask\": tokenized.attention_mask.squeeze(0),\n",
    "            \"original_data\": item # Keep original data for ground truth comparison\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfe05b",
   "metadata": {},
   "source": [
    "Setting up model and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b2a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Model and tokenizer\n",
    "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b6eb4",
   "metadata": {},
   "source": [
    "Add special tokens if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_special_tokens = tokenizer.all_special_tokens\n",
    "needed_markers = [\"<spot>\", \"<asoc>\"]\n",
    "tokens_to_add = [tok for tok in needed_markers if tok not in current_special_tokens]\n",
    "\n",
    "if tokens_to_add:\n",
    "    print(f\"Adding special tokens: {tokens_to_add}\")\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': tokens_to_add})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(\"Resized model embeddings.\")\n",
    "else:\n",
    "    print(\"Special tokens already present in the tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e7c7f",
   "metadata": {},
   "source": [
    "Setting model to evaluation mode and moving to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6693d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b927225",
   "metadata": {},
   "source": [
    "Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to test files\n",
    "DATASET_FILES = {\n",
    "    \"biored\": \"/path/to/your/biored_test.jsonl\",\n",
    "    \"ddi\": \"/path/to/your/ddi_test.jsonl\",\n",
    "    \"chemprot\": \"/path/to/your/chemprot_test.jsonl\",\n",
    "    \"bc5cdr\": \"/path/to/your/bc5cdr_test.jsonl\",\n",
    "}\n",
    "\n",
    "MAX_SOURCE_LENGTH = 512 # adjust based on the data/model\n",
    "MAX_TARGET_LENGTH = 512 # adjust based on model\n",
    "BATCH_SIZE = 16\n",
    "PREDICTIONS_DIR = \"./predictions/uie\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c74ac",
   "metadata": {},
   "source": [
    "Loop through datasets and save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
    "print(f\"Saving predictions to: {PREDICTIONS_DIR}\")\n",
    "\n",
    "# Loop through datasets for inference\n",
    "for dataset_name, test_file in DATASET_FILES.items():\n",
    "    print(f\"\\n--- Processing Dataset: {dataset_name} ---\")\n",
    "\n",
    "    # Load data\n",
    "    try:\n",
    "        test_data = load_jsonl(test_file)\n",
    "        print(f\"Loaded {len(test_data)} examples.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Test file not found: {test_file}. Skipping dataset.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {test_file}: {e}. Skipping dataset.\")\n",
    "        continue\n",
    "\n",
    "    # Get mapper and build SSI\n",
    "    mapper = BIO_LABEL_MAPPERS.get(dataset_name)\n",
    "    if not mapper:\n",
    "        print(f\"Label mapper not found for {dataset_name}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    entity_map = mapper['entities']\n",
    "    relation_map = mapper['relations']\n",
    "    entity_types_sorted = sorted(list(entity_map.values()))\n",
    "    relation_types_sorted = sorted(list(relation_map.values()))\n",
    "    ssi_string = build_ssi(entity_types_sorted, relation_types_sorted)\n",
    "    print(\"Generated SSI string:\", ssi_string)\n",
    "\n",
    "    # Create dataLoader\n",
    "    inference_dataset = InferenceDataset(test_data, tokenizer, ssi_string, MAX_SOURCE_LENGTH)\n",
    "    dataloader = DataLoader(inference_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    output_filepath = os.path.join(PREDICTIONS_DIR, f\"{dataset_name}.jsonl\")\n",
    "\n",
    "    # Run inference and write predictions\n",
    "    with torch.no_grad(), open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "        for batch in tqdm(dataloader, desc=f\"Inferring & Saving {dataset_name}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "            )\n",
    "\n",
    "            # Decode, parse SEL\n",
    "            sel_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n",
    "            cleaned_sels = [s.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\").strip() for s in sel_outputs]\n",
    "            parsed_preds = [parse_sel_string(s) for s in cleaned_sels]\n",
    "\n",
    "            # Extract sets from parsed predictions and ground truth\n",
    "            batch_preds_sets = [get_predicted_sets(p) for p in parsed_preds]\n",
    "            batch_gts_sets = [get_ground_truth_sets(item, entity_map, relation_map) for item in batch['original_data']]\n",
    "            original_texts = [item['text'] for item in batch['original_data']]\n",
    "\n",
    "            # Write each item in the batch to the output file\n",
    "            for i in range(len(original_texts)):\n",
    "                pred_ents, pred_rels = batch_preds_sets[i]\n",
    "                gt_ents, gt_rels = batch_gts_sets[i]\n",
    "\n",
    "                output_record = {\n",
    "                    \"text\": original_texts[i],\n",
    "                    \"sel_output\": cleaned_sels[i],\n",
    "                    \"predicted_entities\": sorted([list(e) for e in pred_ents]),\n",
    "                    \"predicted_relations\": sorted([list(r) for r in pred_rels]),\n",
    "                    \"ground_truth_entities\": sorted([list(e) for e in gt_ents]),\n",
    "                    \"ground_truth_relations\": sorted([list(r) for r in gt_rels])\n",
    "                }\n",
    "                outfile.write(json.dumps(output_record) + \"\\n\")\n",
    "\n",
    "    print(f\"Finished processing and saved predictions for {dataset_name} to {output_filepath}\")\n",
    "\n",
    "print(\"\\n=== Inference and Saving Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad8441",
   "metadata": {},
   "source": [
    "Get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics = calculate_metrics_from_files(PREDICTIONS_DIR)\n",
    "\n",
    "print(\"\\n=== Final Zero-Shot Results Summary ===\")\n",
    "if final_metrics:\n",
    "    print(json.dumps(final_metrics, indent=2))\n",
    "else:\n",
    "    print(\"No metrics were calculated.\")\n",
    "print(\"=======================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
