{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01bf454",
   "metadata": {},
   "source": [
    "# Universal Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f2c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BatchEncoding, PreTrainedTokenizerBase\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import List, Dict, Set, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb2b2e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72020d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"luyaojie/uie-large-en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8800e",
   "metadata": {},
   "source": [
    "Define special tokens based on UIE's T5 usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48afce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '<extra_id_0>'\n",
    "END = '<extra_id_1>'\n",
    "TARGET = '<extra_id_5>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a3451",
   "metadata": {},
   "source": [
    "#### Define label mappers for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIO_LABEL_MAPPERS = {\n",
    "    \"biored\": {\n",
    "        \"entities\": {\n",
    "            \"GeneOrGeneProduct\": \"gene or gene product\",\n",
    "            \"DiseaseOrPhenotypicFeature\": \"disease or phenotypic feature\",\n",
    "            \"ChemicalEntity\": \"chemical or drug\",\n",
    "            \"SequenceVariant\": \"genomic or protein variant\",\n",
    "            \"OrganismTaxon\": \"species\",\n",
    "            \"CellLine\": \"cell line\",\n",
    "        },\n",
    "        \"relations\": {\n",
    "            \"Association\": \"is associated with\",\n",
    "            \"Positive_Correlation\": \"positively correlates with\",\n",
    "            \"Negative_Correlation\": \"negatively correlates with\",\n",
    "            \"Bind\": \"binds to\",\n",
    "            \"Cotreatment\": \"is cotreated with\",\n",
    "            \"Comparison\": \"is compared to\",\n",
    "            \"Conversion\": \"converts to\",\n",
    "            \"Drug_Interaction\": \"interacts with drug\",\n",
    "        }\n",
    "    },\n",
    "    \"ddi\": {\n",
    "        \"entities\": {\n",
    "            \"DRUG\": \"drug\",\n",
    "            \"GROUP\": \"groups of drugs\",\n",
    "            \"BRAND\": \"drug brand\",\n",
    "            \"DRUG_N\": \"unapproved drug\"\n",
    "        },\n",
    "        \"relations\": {\n",
    "            \"MECHANISM\": \"has mechanism\",\n",
    "            \"EFFECT\": \"has effect\",\n",
    "            \"ADVISE\": \"is advised against\",\n",
    "            \"INT\": \"interacts with\",\n",
    "        }\n",
    "    },\n",
    "    \"chemprot\": {\n",
    "         \"entities\": {\n",
    "            \"CHEMICAL\": \"chemical\",\n",
    "            \"GENE-N\": \"gene or protein\",\n",
    "            \"GENE-Y\": \"gene or protein\",\n",
    "        },\n",
    "        \"relations\": {\n",
    "            \"Agonist\": \"is agonist of\",\n",
    "            \"Antagonist\": \"is antagonist of\",\n",
    "            \"Cofactor\": \"is cofactor of\",\n",
    "            \"Downregulator\": \"downregulates\",\n",
    "            \"Modulator\": \"modulates\",\n",
    "            \"Not\": \"not related to\",\n",
    "            \"Part_of\": \"is part of\",\n",
    "            \"Regulator\": \"regulates\",\n",
    "            \"Substrate\": \"is substrate of\",\n",
    "            \"Undefined\": \"unkown relation to\",\n",
    "            \"Upregulator\": \"upregulates\",\n",
    "        }\n",
    "    },\n",
    "    \"bc5cdr\": {\n",
    "        \"entities\": {\n",
    "            \"Chemical\": \"chemical\",\n",
    "            \"Disease\": \"disease\",\n",
    "        },\n",
    "        \"relations\": {\n",
    "            \"CID\": \"causes or induces\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81c4ec",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c1cc8",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ec96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads a JSONL file into a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a819b",
   "metadata": {},
   "source": [
    "### UIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d16fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ssi(entity_types: List[str], relation_types: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Builds the Structured Schema Instructor (SSI) string.\n",
    "    Uses <spot> for entities and <asoc> for relations.\n",
    "    \"\"\"\n",
    "    ssi = \"<spot> \" + \"<spot> \".join(entity_types)\n",
    "    ssi += \" <asoc> \" + \"<asoc> \".join(relation_types)\n",
    "    ssi += \" <extra_id_2>\"\n",
    "    return ssi\n",
    "\n",
    "def parse_sel_string(sel_string: str) -> List[Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Parses the cleaned generated SEL string into a structured format using defined markers.\n",
    "    \"\"\"\n",
    "    structured_output = []\n",
    "\n",
    "    # The entire SEL is wrapped in START/END\n",
    "    if sel_string.startswith(START) and sel_string.endswith(END):\n",
    "        sel_string = sel_string[len(START):-len(END)].strip()\n",
    "    else:\n",
    "        return structured_output\n",
    "\n",
    "    # Each block is wrapped in START/END\n",
    "    records: List[str] = []\n",
    "    start_index = sel_string.find(START)\n",
    "\n",
    "    # Handle cases where the string might not contain any records or is malformed\n",
    "    if start_index == -1 and len(sel_string) > 0:\n",
    "        return structured_output\n",
    "\n",
    "    while start_index != -1:\n",
    "        end_index = len(sel_string)\n",
    "        curr_start = start_index\n",
    "        while curr_start < end_index and curr_start != -1 and end_index != -1:\n",
    "            end_index = sel_string.find(END, curr_start + len(START))\n",
    "            curr_start = sel_string.find(START, curr_start + len(START))\n",
    "        \n",
    "        if end_index == -1:\n",
    "            return structured_output\n",
    "        \n",
    "        # Extract the content between START and END\n",
    "        record_content = sel_string[start_index + len(START):end_index].strip()\n",
    "        if record_content:\n",
    "             records.append(record_content)\n",
    "\n",
    "    # Process each record\n",
    "    for record in records:\n",
    "        try:\n",
    "            # Extract entity type\n",
    "            start = record.find(START)\n",
    "            subj_sep = record.find(TARGET)\n",
    "            if not (start == 0 and subj_sep > start):\n",
    "                 continue\n",
    "            \n",
    "            entity_type = record[start + len(START):subj_sep].strip()\n",
    "\n",
    "            # Extract entity span and relations (if any)\n",
    "            remaining_record = record[subj_sep + len(TARGET):].strip()\n",
    "            \n",
    "            # Check if there are associations after the main span\n",
    "            rel_start = remaining_record.find(START)\n",
    "            \n",
    "            subj_span = \"\"\n",
    "            relations = []\n",
    "\n",
    "            if rel_start != -1: # relations exist\n",
    "                subj_span = remaining_record[:rel_start].strip()\n",
    "                while rel_start != -1:\n",
    "                    rel_end = remaining_record.find(END, rel_start + len(START))\n",
    "                    if rel_end == -1:\n",
    "                        break\n",
    "                    \n",
    "                    # Extract the content between START and END\n",
    "                    relation_str = remaining_record[rel_start + len(START):rel_end].strip()\n",
    "                    target_sep = relation_str.find(TARGET)\n",
    "                    if target_sep == -1:\n",
    "                        continue\n",
    "\n",
    "                    rel_type = relation_str[:target_sep].strip()\n",
    "                    obj_span = relation_str[target_sep + len(TARGET):].strip()\n",
    "                    if rel_type and obj_span:\n",
    "                        relations.append((rel_type, obj_span))\n",
    "                    \n",
    "                    rel_start = remaining_record.find(START, rel_end + len(END))\n",
    "\n",
    "            else: # no relations, the rest is the entity span\n",
    "                subj_span = remaining_record\n",
    "\n",
    "            # Add to output if valid\n",
    "            if entity_type and subj_span:\n",
    "                record_info = {'span': subj_span, 'spot': entity_type, 'asoc': relations}\n",
    "                structured_output.append(record_info)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing block: {record}\\nError: {e}\")\n",
    "            continue\n",
    "\n",
    "    return structured_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519bdaa",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cbab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(dataset_name: str, test_file: str, output_path: str) -> None:\n",
    "    try:\n",
    "        test_data = load_jsonl(test_file)\n",
    "        print(f\"Loaded {len(test_data)} examples.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {test_file}: {e}.\")\n",
    "        return -1\n",
    "\n",
    "    # Get mapper and build SSI\n",
    "    mapper = BIO_LABEL_MAPPERS.get(dataset_name)\n",
    "    if not mapper:\n",
    "        print(f\"Label mapper not found for {dataset_name}. Skipping.\")\n",
    "        return -1\n",
    "\n",
    "    entity_map = mapper['entities']\n",
    "    relation_map = mapper['relations']\n",
    "    entity_types_sorted = sorted(list(entity_map.values()))\n",
    "    relation_types_sorted = sorted(list(relation_map.values()))\n",
    "    ssi_string = build_ssi(entity_types_sorted, relation_types_sorted)\n",
    "    print(\"Generated SSI string:\", ssi_string)\n",
    "\n",
    "    # Create dataLoader\n",
    "    inference_dataset = InferenceDataset(test_data, tokenizer, ssi_string, MAX_SOURCE_LENGTH)\n",
    "    dataloader = DataLoader(inference_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    output_filepath = os.path.join(PREDICTIONS_DIR, f\"{dataset_name}.jsonl\")\n",
    "\n",
    "# Run inference and write predictions\n",
    "with torch.no_grad(), open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "    for batch in tqdm(dataloader, desc=f\"Inferring & Saving {dataset_name}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "        )\n",
    "\n",
    "        # Decode, parse SEL\n",
    "        sel_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n",
    "        cleaned_sels = [s.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\").strip() for s in sel_outputs]\n",
    "        parsed_preds = [parse_sel_string(s) for s in cleaned_sels]\n",
    "\n",
    "        # Extract sets from parsed predictions and ground truth\n",
    "        batch_preds_sets = [get_predicted_sets(p) for p in parsed_preds]\n",
    "        batch_gts_sets = [get_ground_truth_sets(item, entity_map, relation_map) for item in batch['original_data']]\n",
    "        original_texts = [item['text'] for item in batch['original_data']]\n",
    "\n",
    "        # Write each item in the batch to the output file\n",
    "        for i in range(len(original_texts)):\n",
    "            pred_ents, pred_rels = batch_preds_sets[i]\n",
    "            gt_ents, gt_rels = batch_gts_sets[i]\n",
    "\n",
    "            output_record = {\n",
    "                \"text\": original_texts[i],\n",
    "                \"sel_output\": cleaned_sels[i],\n",
    "                \"predicted_entities\": sorted([list(e) for e in pred_ents]),\n",
    "                \"predicted_relations\": sorted([list(r) for r in pred_rels]),\n",
    "                \"ground_truth_entities\": sorted([list(e) for e in gt_ents]),\n",
    "                \"ground_truth_relations\": sorted([list(r) for r in gt_rels])\n",
    "            }\n",
    "            outfile.write(json.dumps(output_record) + \"\\n\")\n",
    "\n",
    "print(f\"Finished processing and saved predictions for {dataset_name} to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37058a6",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_sets(data_sample: Dict, entity_map: Dict, relation_map: Dict) -> Tuple[Set[Tuple[str, str]], Set[Tuple[str, str, str]]]:\n",
    "    \"\"\"Extracts ground truth entities and relations into sets for easy comparison.\"\"\"\n",
    "    gt_entities = set()\n",
    "    gt_relations = set()\n",
    "\n",
    "    for entity in data_sample.get('entities', []):\n",
    "        entity_span = entity['text']\n",
    "        entity_type = entity_map.get(entity['type'], entity['type'])\n",
    "        gt_entities.add((entity_span, entity_type))\n",
    "    \n",
    "    for relation in data_sample.get('relations', []):\n",
    "        head_span = relation['head']['text']\n",
    "        tail_span = relation['tail']['text']\n",
    "        relation_type = relation_map.get(relation['type'], relation['type'])\n",
    "        gt_relations.add((head_span, relation_type, tail_span))\n",
    "\n",
    "    return gt_entities, gt_relations\n",
    "\n",
    "def get_predicted_sets(parsed_sel: List[Dict['str', any]]) -> Tuple[Set[Tuple[str, str]], Set[Tuple[str, str, str]]]:\n",
    "    \"\"\"Extracts predicted entities and relations from the parsed SEL structure.\"\"\"\n",
    "    pred_entities = set()\n",
    "    pred_relations = set()\n",
    "\n",
    "    for record in parsed_sel:\n",
    "        subj_span = record['span']\n",
    "        subj_type = record['spot']\n",
    "        pred_entities.add((subj_span, subj_type))\n",
    "        for rel_type, obj_span in record['asoc']:\n",
    "            pred_relations.add((subj_span, rel_type, obj_span))\n",
    "\n",
    "    return pred_entities, pred_relations\n",
    "\n",
    "def calculate_metrics(preds: List[Tuple[Set, Set]], golds: List[Tuple[Set, Set]]) -> Dict:\n",
    "    \"\"\"Calculates P/R/F1 for entities and relations.\"\"\"\n",
    "    total_ent_tp, total_ent_fp, total_ent_fn = 0, 0, 0\n",
    "    total_rel_tp, total_rel_fp, total_rel_fn = 0, 0, 0\n",
    "\n",
    "    for (pred_ents, pred_rels), (gold_ents, gold_rels) in zip(preds, golds):\n",
    "        # Entity metrics\n",
    "        total_ent_tp += len(pred_ents.intersection(gold_ents))\n",
    "        total_ent_fp += len(pred_ents.difference(gold_ents))\n",
    "        total_ent_fn += len(gold_ents.difference(pred_ents))\n",
    "\n",
    "        # Relation metrics\n",
    "        total_rel_tp += len(pred_rels.intersection(gold_rels))\n",
    "        total_rel_fp += len(pred_rels.difference(gold_rels))\n",
    "        total_rel_fn += len(gold_rels.difference(pred_rels))\n",
    "\n",
    "    ent_precision = total_ent_tp / (total_ent_tp + total_ent_fp) if (total_ent_tp + total_ent_fp) > 0 else 0\n",
    "    ent_recall = total_ent_tp / (total_ent_tp + total_ent_fn) if (total_ent_tp + total_ent_fn) > 0 else 0\n",
    "    ent_f1 = 2 * (ent_precision * ent_recall) / (ent_precision + ent_recall) if (ent_precision + ent_recall) > 0 else 0\n",
    "\n",
    "    rel_precision = total_rel_tp / (total_rel_tp + total_rel_fp) if (total_rel_tp + total_rel_fp) > 0 else 0\n",
    "    rel_recall = total_rel_tp / (total_rel_tp + total_rel_fn) if (total_rel_tp + total_rel_fn) > 0 else 0\n",
    "    rel_f1 = 2 * (rel_precision * rel_recall) / (rel_precision + rel_recall) if (rel_precision + rel_recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"entity_precision\": ent_precision,\n",
    "        \"entity_recall\": ent_recall,\n",
    "        \"entity_f1\": ent_f1,\n",
    "        \"relation_precision\": rel_precision,\n",
    "        \"relation_recall\": rel_recall,\n",
    "        \"relation_f1\": rel_f1,\n",
    "    }\n",
    "\n",
    "def calculate_metrics_from_file(file_path: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Reads prediction files from the directory and calculates metrics for each.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    all_preds_sets = []\n",
    "    all_labels_sets = []\n",
    "    line_count = 0\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line_count += 1\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    # Convert saved lists back to sets of tuples\n",
    "                    pred_entities = set(tuple(e) for e in data['pred_entities'])\n",
    "                    pred_relations = set(tuple(r) for r in data['pred_relations'])\n",
    "                    gt_entities = set(tuple(e) for e in data['gt_entities'])\n",
    "                    gt_relations = set(tuple(r) for r in data['gt_relations'])\n",
    "\n",
    "                    all_preds_sets.append((pred_entities, pred_relations))\n",
    "                    all_labels_sets.append((gt_entities, gt_relations))\n",
    "                \n",
    "                except json.JSONDecodeError:\n",
    "                        print(f\"Skipping malformed JSON line {line_count} in {file_path}\")\n",
    "\n",
    "        metrics = calculate_metrics(all_preds_sets, all_labels_sets)\n",
    "    \n",
    "    except Exception as e:\n",
    "            print(f\"Error reading or processing {file_path}: {e}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9049169",
   "metadata": {},
   "source": [
    "## Zero-Shot Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740c570",
   "metadata": {},
   "source": [
    "Dataset Class for Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UIEDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict[str, any]], tokenizer: AutoTokenizer, ssi_string: str, max_source_length: int = 512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ssi_string = ssi_string\n",
    "        self.max_source_length = max_source_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        input_text = f\"{self.ssi_string} {text}\"\n",
    "        tokenized = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_source_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokenized.input_ids.squeeze(0), # squeeze to remove batch dim added by tokenizer\n",
    "            \"attention_mask\": tokenized.attention_mask.squeeze(0),\n",
    "            \"original_data\": item # original data for ground truth comparison\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfe05b",
   "metadata": {},
   "source": [
    "Setting up model and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b2a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b6eb4",
   "metadata": {},
   "source": [
    "Add special tokens if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = tokenizer.all_special_tokens\n",
    "needed_markers = [\"<spot>\", \"<asoc>\"]\n",
    "tokens_to_add = [tok for tok in needed_markers if tok not in special_tokens]\n",
    "\n",
    "if tokens_to_add:\n",
    "    print(f\"Adding special tokens: {tokens_to_add}\")\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': tokens_to_add})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(\"Resized model embeddings.\")\n",
    "else:\n",
    "    print(\"Special tokens already present in the tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e7c7f",
   "metadata": {},
   "source": [
    "Setting model to evaluation mode and moving to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6693d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b927225",
   "metadata": {},
   "source": [
    "Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c9d4e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c74ac",
   "metadata": {},
   "source": [
    "### ChemProt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfed066",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d5567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffad8441",
   "metadata": {},
   "source": [
    "Get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics = calculate_metrics_from_file(PREDICTIONS_DIR)\n",
    "\n",
    "print(\"\\n=== Final Zero-Shot Results Summary ===\")\n",
    "if final_metrics:\n",
    "    print(json.dumps(final_metrics, indent=2))\n",
    "else:\n",
    "    print(\"No metrics were calculated.\")\n",
    "print(\"=======================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
